#  Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data

code:https://github.com/BerkeleyAutomation/sd-maskrcnn

总结
感觉这篇文章就是把maskrcnn用在了depth上，大数据的训练实现了class agnostic。本文的一个亮点是提供了一个数据集，该数据集既有合成的，又有对应真实数据集。
既然简单地分成了前景和背景，那么其他的检测？分割的方法也可以这么做。
而且只是要简单地分割成前景背景，太多的rgb信息反而容易形成混淆。深度图训练效果会比较好。

3.2.1 WISDOM-Sim和WISDOM-Real
3.2.2 Synthetic Depth MASK R-CNN(SD Mask-rcnn)
1 将深度图看成灰度图，改成三通道。
2 将类分成两类，前景和背景
3 depth输入改成512*512
4 网络主框架从ResNet 101改成ResNet 35
5 设置mean pixel value

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200418114548561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2VpZ2h0X0plc3Nlbg==,size_16,color_FFFFFF,t_70)

从这张图看，但训练的图片越多，还有类别越多，效果会越好。




# Deep Learning for RGB-D Image Segmentation

![image-20230710231950617](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230710231950617.png)

## 从 RGB-D 图像中学习丰富的特征以进行对象检测和分割，ECCV 2014

概述：我们的系统从 RGB 和深度图像对中检测轮廓，生成 2.5D 区域建议，将它们分类为对象类别，然后推断“事物”类对象实例的分割掩模，以及属于以下对象的像素的标签：类似“东西”的类别。

我们建议在每个像素处使用三个通道对深度图像进行编码：水平视差**(horizontal disparity)**、地面高度**(height above ground)**以及像素局部表面法线与推断重力方向形成的角度。我们将此编码称为 **HHA**。所有三个通道均进行线性缩放，以将训练数据集中的观测值映射到 0 到 255 的范围。

HHA 表示对地心姿态的属性进行编码，强调图像中的互补不连续性（深度、表面法线和高度）。此外，CNN 不太可能自动学习直接从深度图像计算这些属性，特别是当可用的训练数据非常有限时。

我们的假设是，为 RGB 图像设计的网络也可以学习 HHA 图像的合适表示。

## FuseNet：通过基于融合的 CNN 架构将深度融入语义分割

随着 RGB-D 相机的出现，预计额外的深度测量将提高精度。在这里，我们研究了如何利用卷积神经网络（CNN）将互补深度信息合并到语义分割框架中的解决方案。最近，编码器-解码器类型的全卷积 CNN 架构在语义分割领域取得了巨大成功。受此观察的启发，我们提出了一种编码器-解码器类型的网络，其中编码器部分由网络的两个分支组成，这两个分支同时从 RGB 和深度图像中提取特征，并随着网络的深入将深度特征融合到 RGB 特征图中。

![image-20230711172128459](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711172128459.png)

FuseNet 的示例输出。从左到右：输入RGB和深度图像，预测的语义标签和对应标签的概率，其中白色和蓝色分别表示高概率和低概率![image-20230711172516812](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711172516812.png)

所提出的 FuseNet 的架构。颜色表示图层类型。该网络包含两个分支，用于从 RGB 和深度图像中提取特征，并且深度的特征图不断融合到 RGB 分支中，用红色箭头表示。在我们的架构中，融合层被实现为逐元素求和，如虚线框中所示。

FuseNet 由编码器-解码器类型的网络架构组成。该网络有两个主要部分：

1. 编码器部分提取特征；
2.  解码器部分将特征映射上采样回原始输入分辨率。

这种编码器-解码器风格已经在 DeconvNet 和 SegNet 等之前的一些工作中引入，并取得了良好的分割性能。此外，两个分支从 RGB 和深度图像中提取特征。我们注意到深度图像被归一化为与彩色图像具有相同的值范围，即进入 [0,255] 区间。为了结合来自两个输入模块的信息，我们将深度分支的特征图融合到 RGB 分支的特征图中。

FuseNet 的编码器部分类似于 16 层 VGG 网络，除了全连接层 fc6、fc7 和 fc8 之外，因为全连接层将分辨率降低了 49 倍，这增加了上采样部分的难度。在我们的网络中，我们总是在卷积（Conv）之后和修正线性单元1（ReLU）之前使用批量归一化（BN）来减少内部协变量偏移。我们将卷积、批量归一化和 ReLU 的组合分别称为 CBR 块。 BN 层首先将特征图归一化为零均值和单位方差，然后对其进行缩放和移位。特别是，尺度和平移参数是在训练期间学习的。因此，颜色特征不会被深度特征覆盖，但网络会学习如何以最佳方式组合它们。

解码器部分是编码器部分的对应部分，其中应用记忆的反池化来对特征图进行上采样。在解码器部分，我们再次使用 CBR 块。我们还用反卷积代替卷积进行了实验，并观察到非常相似的性能。

![image-20230711172713300](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711172713300.png)

VGG 16 层网络的第二个 (CBR2) 和第三个 (CBR3) 卷积块的不同融合策略的图示。 (a) 融合层仅插入在每个池化层之前。 (b) 在每个 CBR 块之后插入融合层。![image-20230711172739288](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711172739288.png)

Huang 等人提出的基于 3DCNN 的点云语义标记系统。云经历密集体素化过程，CNN 生成每体素标签，然后将其映射回点云。

## Deep Projective 3D Semantic Segmentation深度投影 3D 语义分割，2017

3D 点云的语义分割对于许多实际应用来说是一个具有挑战性的问题。虽然深度学习彻底改变了图像语义分割领域，但迄今为止其对点云数据的影响有限。最近基于 3D 深度学习方法 (3DCNN) 的尝试取得了低于预期的结果。此类方法需要对底层点云数据进行体素化，从而导致空间分辨率降低和内存消耗增加。此外，3D-CNN 还受到注释数据集可用性有限的影响。

在本文中，我们提出了一种替代框架，可以避免 3D-CNN 的局限性。我们没有直接解决 3D 问题，而是首先将点云投影到一组合成 2D 图像上。然后将这些图像用作 2D-CNN 的输入，该 2D-CNN 专为语义分割而设计。最后将得到的预测分数重新投影到点云上，得到分割结果。我们进一步研究了多流网络架构中多种模式的影响，例如颜色、深度和表面法线。![image-20230711172953685](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711172953685.png)

所提出方法的概述。输入点云被投影到多个虚拟摄像机视图中，生成 2D 颜色、深度和表面法线图像。每个视图的图像由多流 CNN 处理以进行语义分割。所有视图的输出预测分数被融合为每个点的单个预测，从而产生点云的 3D 语义分割。

![image-20230711173048393](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711173048393.png)

所提出的用于 2D 语义分割的多流架构的图示。每个输入流都由全卷积网络处理。将每个流的预测分数相加即可得到最终预测。

## Depth-aware CNN for RGB-D Segmentation, 2018

由于固定的网格内核结构，卷积神经网络（CNN）由于缺乏处理几何信息的能力而受到限制。深度数据的可用性使得使用 CNN 的 RGB-D 语义分割取得了进展。最先进的方法要么使用深度作为附加图像，要么处理 3D 体积或点云中的空间信息。这些方法的计算和内存成本很高。

为了解决这些问题，我们通过引入两种直观、灵活和有效的操作来提出深度感知 CNN：深度感知卷积和深度感知平均池化。通过利用信息传播过程中像素之间的深度相似性，几何形状可以无缝地融入到 CNN 中。在不引入任何额外参数的情况下，这两个算子都可以轻松集成到现有的 CNN 中。

![image-20230711173224678](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711173224678.png)

深度感知 CNN 的插图。 A 和 C 被标记为桌子，B 被标记为椅子。它们在 RGB 图像中都具有相似的视觉特征，但在深度上是可分离的。深度感知 CNN 在卷积和池化中结合了像素的几何关系。当A是感受野的中心时，C对输出单元的贡献比B更大。最右一列的图显示了深度感知CNN的RGBD语义分割结果。

引入了两个新运算符：（a）深度感知卷积和（b）深度感知平均池化。

深度感知卷积通过深度相似性项增强了标准卷积。我们强制与内核中心深度相似的像素比其他像素对输出有更多贡献。这个简单的深度相似性项有效地将几何结构合并到卷积核中，并有助于构建深度感知的感受野，其中卷积不受固定网格几何结构的限制。

第二个引入的运算符是深度软件平均池化。类似地，当对特征图的局部区域应用滤波器时，在计算局部区域的平均值时考虑相邻像素之间的深度成对关系。视觉特征能够沿着深度图像中给出的几何结构传播。这种几何感知操作使得能够利用深度图像来定位对象边界。

![image-20230711173302403](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711173302403.png)

深度感知 CNN 中信息传播的图示。不失一般性，我们只显示一个核大小为 3 × 3 的滤波器窗口。在图中所示的深度相似度中，较深的颜色表示相似度较高，而较浅的颜色表示两个像素在深度上相似度较低。在（a）中，深度感知卷积的输出激活是深度相似度窗口与输入特征图上的卷积窗口的乘积。与(b)中类似，深度感知平均池化的输出是根据深度相似度加权的输入窗口的平均值。

## Cascaded Feature Network for Semantic Segmentation of RGB-D Images, ICCV 

## 用于 RGB-D 图像语义分割的级联特征网络，ICCV 2017![image-20230711173349462](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711173349462.png)

深度和场景分辨率之间存在相关性：近场（以蓝色矩形突出显示）包含高场景分辨率，而远场（以红色矩形突出显示）具有低场景分辨率。这项工作的关键思想是使用深度将图像分割成代表相似视觉特征的层，或“场景分辨率”，例如一般物体和场景的分辨率。

深度和场景分辨率之间存在相关性：较低的场景分辨率出现在具有较高深度的区域中，而较高的场景分辨率出现在近场中。在较低场景分辨率区域中，对象和场景密集共存，相对于较高场景分辨率区域，对象/场景之间形成更复杂的相关性。为了更好地表示和学习不同的对象/场景关系，应该为不同的场景分辨率构建适当的特征。

首先，为了使特征更加关注观察场景的共同视觉特征，引入了上下文感知感受野（CaRF）。 CaRF 可以更好地控制所学习特征的相关上下文信息。CaRF 是基于超像素计算的，超像素由底层场景结构定义。因此，CaRF提供的上下文信息可以减轻混合过小或过大区域特征的负面影响。

其次，开发了具有并行分支的级联特征网络（CFN），每个分支都专注于特定场景分辨率区域的语义分割。每个分支都配备了 CaRF。 CaRF和级联网络的结合，使得不同场景分辨率的区域能够相互通信，从而明智地更新共享的卷积特征。![image-20230711173657074](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711173657074.png)

我们的级联特征网络（CFN）的概述。给定彩色图像，我们使用 CNN 计算卷积特征图。离散深度图像是分层的，其中每一层代表场景分辨率，并用于将图像区域与共享相同卷积特征图的相应网络分支进行匹配。每个分支都有上下文感知感受野（CaRF），它产生上下文表示以与相邻分支的特征相结合。所有分支的预测被结合起来以获得最终的分割结果。

![image-20230711173726757](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711173726757.png)

两级上下文感知感受野（CaRF）：（a）将图像划分为不同大小的超像素； (b) 在粗网格的每个节点，我们聚合驻留在同一超像素中的特征； (c)聚合相邻超像素的内容； (d) 特征图中的聚合内容表示 CaRF。两级 CaRF 被重复应用于由不同尺寸的超像素划分的图像。请注意，由于网络的下采样，特征图的分辨率小于图像。

![image-20230711173752578](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711173752578.png)

该网络可以具有单独的分支（a）、组合分支（b）或级联分支（c）。为了清楚起见，我们仅用两个分支进行说明。每个网络都可以扩展以拥有更多分支。

上下文感知感受野（CaRF）的思想是将局部上下文的卷积特征聚合成更丰富的特征，以更好地学习相关内容，其中感受野是空间变化的，并根据局部上下文定义其范围。![image-20230711173827462](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711173827462.png)

## Multi-View Deep Learning for Consistent Semantic Mapping with RGB-D Cameras, 使用 RGB-D 相机进行一致语义映射的多视图深度学习,2017

开发了一种新颖的深度学习方法，用于对具有多视图上下文的 RGB-D 图像进行语义分割，其中 RGB 和深度融合通过多尺度深度监督和多视图一致性约束无缝融合。一个共同的原则是使用 SLAM 轨迹估计将多个帧的网络输出扭曲为带有地面实况注释的参考视图。通过这种方式，网络可以学习在视点变化下不变的特征。![image-20230711173919926](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711173919926.png)

关键的创新是通过使用 SLAM 轨迹将 CNN 特征图从多个视图变形为公共参考视图来增强一致性，并监督多个尺度的训练。我们的方法提高了单视图分割的性能，并且特别有利于多视图融合分割。

![image-20230711174046296](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711174046296.png)

我们的方法中使用的 CNN 编码器-解码器架构。网络的输入是 RGB-D 序列以及来自 SLAM 轨迹的相应姿势。受 FuseNet 启发，编码器包含两个分支来从 RGB-D 数据中学习特征。

获得的低分辨率高维特征图通过解码器中的反卷积连续细化。我们将特征映射转换为公共参考视图，并通过各种约束强制执行多视图一致性。网络以深度监督的方式进行训练，其中在解码器的所有尺度上计算损失。

![image-20230711174111290](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711174111290.png)

![image-20230711174147594](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711174147594.png)

具有来自 RGB 和下采样 HHA 编码深度的 CNN 特征的检测管道。 C 表示最后一个卷积层之后的 CNN 特征图数量（VGG-16 为 512）。内部提案生成器生成 B 提案（1000）。

![image-20230711174230251](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711174230251.png)

具有来自 RGB 和 HHA 编码深度的级联 CNN 特征的检测管道。 C 表示最后一个卷积层之后的 CNN 特征图数量（VGG16 为 512）。内部提案生成器生成 B 提案（1000）。对于跨模态蒸馏方法，训练 CNN ψ 来模仿预训练的 CNN φ。![image-20230711174259627](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711174259627.png)

## RGB-D Face Recognition via Deep Complementary and Common Feature Learning, 通过深度互补和共同特征学习进行 RGB-D 人脸识别，2018

![image-20230711174332165](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711174332165.png)

RGB-D人脸识别（FR）由两种典型的场景组成：（1）多模态匹配，例如RGB-D探针与RGB-D图库，其中注册图库和探针图像都是使用RGBD传感器捕获的， (2) 跨模态匹配，例如 RGB 探针与 RGB-D 图库，其中图库图像保持 RGB-D，但探针图像由 RGB 传感器捕获。所提出的方法通过学习互补和共同特征来解决这两个问题。

![image-20230711174402554](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711174402554.png)

概述了所提出的 RGB 和深度模态的互补特征学习方法，该方法处理多模态 FR 场景，例如 RGB-D 探针与 RGB-D 图库。

![image-20230711174518016](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711174518016.png)

我们的互补特征学习的网络架构

![image-20230711174540174](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711174540174.png)

引入loss4单元的细节是为了加强互补特征学习。加号表示逐元素平均值。



# Image Segmentation Based on Histogram of Depth and an Application in Driver Distraction Detection 

摘要：本研究提出了一种基于深度值直方图从深度图像中分割人体对象的方法。首先根据直方图区域的预定义阈值提取感兴趣区域。然后采用区域生长过程来分离具有相同深度间隔的多个人体。我们的贡献是根据检测到的直方图区域识别自适应生长阈值。为了证明该方法的有效性，介绍了驾驶员分心检测中的应用。

成功提取驾驶员在车内的位置后，我们提出了一个简单的解决方案来跟踪驾驶员的运动。

通过分析初始帧和当前帧之间的差异，感兴趣区域中的簇位置或深度值的变化超过预设阈值，被认为是分散注意力的活动。实验结果表明，该算法成功检测出典型的分心驾驶活动，例如使用手机打电话或发短信、调整内部设备以及实时饮酒等。

## I. 简介

 图像中的人体分割和识别是当今计算机视觉领域的关键挑战。随着 Kinect 相机 [1] 的发布，研究人员能够开发基于深度信息的新解决方案。 [2] 中全面回顾了几种方法。 Spinello 在 [3] 中引入了一种基于 RGB 数据中定向深度直方图和定向梯度直方图的组合检测器。该方法被证明可以在不同的视觉混乱条件下检测不同范围内的多个人。 Choi 也提出了类似的想法，将多种基于图像和基于深度的方法融合在一起，例如行人和上半身、面部、皮肤、形状和运动检测器 [4]。

还可以使用背景减法算法从深度图像中分割人体[5] [6]。从训练集中获得深度背景图像。如果当前帧的像素与对应背景像素之间的差异超过预定义阈值，则该像素被视为前景，该阈值是根据每个像素的平均值和标准差计算的。

Lu Xia 提出了一种基于模型的方法，通过使用 2D 倒角距离匹配和 3D 模型拟合来将人体与周围环境分割开来 [7]。然后通过区域生长算法准确地提取分割身体的轮廓，并在我们的研究中再次应用并进行修改以获得更稳健的结果。

利用表示感兴趣对象的深度值应在特定范围内平滑变化的事实，我们在本文中采用深度直方图来提取对象并将其与其周围环境分离。由于直方图描述的是重复次数，而不是深度值的空间分布，因此它只能分割一组具有相同深度范围的对象。为了克服这个问题，应用区域生长算法来确定潜在对象的边界框。然后使用边界框的比例来识别人体。所提出的方法应用于分心驾驶检测应用，其中在第一帧中提取驾驶员位置并用作参考。当前帧和参考帧之间的比较提供了分析运动和深度值变化的信息。

本文的结构如下。第 2 节介绍了基于直方图的分割。第 3 节描述了分心驾驶检测的应用。最后两节介绍了实验和结论。

## II. 基于直方图的分割 

### A. 感兴趣区域 (ROI) 检测 

分析深度图像以确定出现的深度值及其相应的重复次数。表示为零值的数据噪声将被忽略。令 $X $ 为升序排序深度值的向量$X=[x_1,x_2,..,x_n](x_1<x_2<...<x_n)$,Y 是每个测量 $Y=[y_1,y_2,...,y_n]$的相应重复的向量。如果元素 $Y_i$的值大于其两个邻居，则将其视为峰值：
$$
\begin{cases}
Y_i >Y_{i-1}\\
Y_i>Y_{i+1}
\end{cases}
\tag{1}
$$
<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230731151635579.png" alt="image-20230731151635579" style="zoom: 67%;" />

- ​			图 1. 深度和峰值检测的直方图 (a) 样本场景的深度图像 (b) 其相应的带有峰值的直方图

![image-20230731151751836](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230731151751836.png)

- ​			图 2. 基于 HOD 的分割过程 (a) 具有三个 ROI 的直方图 (b) ROI 变宽

图 1a 展示了两个男人站在平坦背景前的场景。图 1b 显示了深度和检测到的峰值的直方图。此阶段的峰值数量和深度值的分布提供了噪声信息来识别感兴趣的区域。人体的直方图形状也会根据他的姿势而变化。因此，使用局部峰值来分割图像对于深度数据可能无效。设 $T$ 为人体最近距离和最远距离之差（通常为 $T=40cm$ ），$P=[p_1,p_2,...,p_k](k<n)$为包含检测到的峰值的向量。对得到的深度图进行分割的算法如下：

----------------------

**For i=1:k**

​	**if**
$$
|X(p_{i+1})-X(p_i)|>T \tag{2}
$$

- (i)将峰值数组分成两部分  

$$
P_1=[p_1,p_2,...,p_i],P_2=[p_{i+1},p_{i+2},...,p_{k}]
$$



- (ii)对于每个新part，按 (1) 中的方式计算新峰值，以减少意外峰值

--------------------

在部分数组上重复分割和归约过程，直到只剩下一个峰值或剩余区域的宽度小于定义的阈值 $T$：
$$
\{length(P)<2 \\
 |P(first)-P(last)|<T
 \tag{3}
$$
图2a显示了分割过程，其中直方图被分为三个区域。为了避免信息丢失，检测到的区域从两侧变宽$T/2$，然后缩小到最近的谷，如图 2 所示。 2b.

### B. 自适应区域生长算法(Adaptive Region Growing Algorithm)

<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230731154817771.png" alt="image-20230731154817771" style="zoom:67%;" />



- 图 3 区域生长算法在深度值区间相同的两个目标上的性能 (a) 通过峰值检测提取的 ROI (b) 使用种子点和自适应阈值的最终提取结果

在每个检测到的区域上，区域生长过程应用于具有相同深度间隔的单独对象。与[7]类似，生长过程算法的主要因素是种子位置和生长阈值。对每个检测到的区域进行垂直和水平扫描，以定位深度值密度最高的位置。该位置很有可能成为感兴趣的对象，因此被选为种子点。两个像素 x 和 y 之间的相似度$S(x,y)$的定义与[7]中类似：
$$
S(x,y)=|depth(x)-depth(y)|
\tag{4}
$$
阈值是根据检测区域的深度直方图 (HoD) 确定的。与[7]不同，本研究没有设置固定的生长阈值，因为每个对象上的深度点的密度可能不同，因此如果感兴趣的对象改变其位置，可能会导致错误识别。相反，对于每个找到的种子点 $SP$,确定 $HoD$ 中相应柱 $X_{SP}$ 的位置。然后通过比较 $SP$ 与其邻居的深度来定义添加每个种子$S$ 的邻居像素的阈值：
$$
T_H=\mathbf {max}(X_{SP+1}-X_{SP},X_{SP}-X_{SP-1})
\tag{5}
$$
由于每个种子的深度值随其位置而变化，因此阈值$T_H$不是固定的，而是相应地进行调整。结果，该区域生长过程确保所有添加的相邻像素属于检测到的区域的两个连续的现有深度层。该过程的效率得到提高。我们称之为**自适应区域生长算法.**

图3a示出了通过分割过程II.A提取的感兴趣区域，其中两个人没有被分开。图3b显示了种子点检测和高级区域生长算法的结果。由于自适应增长阈值，每个人都被正确提取。

## 三．驾驶员分心检测中的应用(略.)

### A.驾驶员位置识别

采用第二节中的分割算法来检测驾驶员的位置和运动。首先，对车内环境的深度图进行分割。然后使用边界框评估提取区域的形状。它的大小和长宽比是识别驾驶员位置的因素。由于驾驶员将留在驾驶员座椅上，因此仅分析相应边界框内的深度变化来检测驾驶员的运动。将过程限制在边界框内不仅可以消除噪声，还可以减少计算时间而不影响结果。

### B.运动跟踪

通过比较初始帧和当前帧之间的深度值的变化来提取运动信息。[10]中提到了类似的想法，但关注的是每对连续深度帧之间的差异。在我们的工作中，我们假设驾驶员在初始帧中的姿势是驾驶的安全位置。因此，当通过从当前深度帧中减去初始值来跟踪运动时，深度变化提供了更多用于评估的信息。更具体地说，我们分析了调整部分的大小和深度值，同时驾驶员手离开方向盘，做出一些典型的分心活动。

令 $A_R$ 为参考帧中属于驾驶员区域的像素总数。将连通分量算法应用于当前帧和初始帧之间的相减结果，以识别像素值大于零的区域。这些区域的相对大小$A_ {changed}$是通过将其像素数$A_C$除以$A_R$来计算的，如(6)中所示。每个区域的平均深度$D_ {changed}$也按(7)计算：
$$
A_ {changed}=\frac{A_C}{A_R}100\% \tag{6}\\
$$

$$
D_ {changed}=\frac{\sum^{A_c}_{i=1}depth(x_i,y_i)}{A_C}100\% \tag{7}
$$

对于每个被调查的帧，都会记录尺寸的总变化和深度变化的平均值，以便针对驾驶员分心的活动发出警告消息。

## 四．实验结果 

我们使用 Kinect 摄像头录制测试场景的视频序列。仅在车内实验中，摄像头被设置为近距离模式，以避免驾驶员的手距离 Kinect 太近时丢失信息。其他研究是使用 Kinect 的默认范围进行的。该算法在 MATLAB 中实现。

### A. 对象分割

 在第一个实验中，出现了 4 个人，与相机的距离不同（图 3a）。使用HoD分割的区域如图2所示。 3b.最近的人在区域 1 中被检测到，而其他人则出现在区域 2 中。图 3c 显示了前景对象分离过程的结果。属于参与成员的所有部件均已成功提取。当他们改变姿势或位置时，分割结果仍然正确。

<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230731155905646.png" alt="image-20230731155905646" style="zoom:67%;" />

<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230731155914433.png" alt="image-20230731155914433" style="zoom:67%;" />

- 图 4. 4 名男子的测试场景

### B. 深度变化分析

#### 1）图像减影

 在本实验中，驾驶员被要求执行与分心驾驶活动相关的动作，例如发短信、打电话、喝酒和操纵车辆仪表。分析当前帧和参考帧之间的深度变化以确定运动路径。深度变化的值也被考虑在内，如图 2 所示。图5a中，颜色由暗到亮对应于从大到小的变化。

在第二个实验中，Kinect 摄像头安装在汽车内并指向驾驶员。涉及两辆不同的汽车和司机。图 4a 显示了 4 座汽车的原始深度图像和分段驾驶员姿势。 7 座汽车的结果如图 2 所示。 4b.

<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230731161206817.png" alt="image-20230731161206817" style="zoom: 67%;" />

- 图 5. 图像减法示例（开车时通话） (a) 提取的深度图像 (b) 差分深度图像

本例中深度图像的大小为 370x460，而不是原始图像中的 480x640，因为我们仅跟踪提取的驱动程序对象的边界框内的变化。为了更好地解释，我们将差异图像重新校准为灰度图像，其中未改变的像素取值从0到100，改变的像素在150到255的范围内，如图2所示。 5b.

#### 2) Noise Removing 

如图所示。如图5b所示，在驾驶员座椅和汽车天花板等预计不会发生变化的位置出现了一些红点。这些噪声可能会影响评估结果。在本文中，通过在较低分辨率的网格图中表示数据来部分解决这个问题，其中每个单元接收内部深度像素中的最高值。较低分辨率下的运动分析结果如图 2 所示。 6.

<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230731161425247.png" alt="image-20230731161425247" style="zoom:67%;" />

<img src="C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230731161442156.png" alt="image-20230731161442156" style="zoom:67%;" />

- 图 6. 较低分辨率下的运动分析（a 和 b） 开车时打电话（c 和 d） 开车时喝酒（e 和 f） 开车时发短信（g 和 h） 开车时操作车辆仪表

为了将我们的算法与[7]中的算法在分离人脚和局部地平面方面进行比较，我们建立了一个人站在地板上的实验，如图 1 所示。 7a.不幸的是，我们的方法也无法应对这一挑战（图 7b）。造成这种失败的根本原因是所获取的人脚和局部地平面的深度值属于同一深度层。在这种情况下，最好的方法应该是平面检测方法，例如RANSAC平面拟合[11]或V视差图像[12]、[13]。

# Human Detection Using Depth Information by Kinect [通过 Kinect 使用深度信息进行人体检测]

在本文中，我们提出了一种基于模型的新型深度图像人体检测方法。我们的方法使用 Kinect 在室内环境中获得的深度信息来检测人员。我们使用两阶段头部检测过程来检测人员，其中包括 2D 边缘检测器和 3D 形状检测器，以利用深度图像中的边缘信息和相关深度变化信息。我们还提出了一种分割方法，将人物从附着的背景物体中分割出来，并准确地提取主体的整体轮廓。该方法在我们实验室使用 Xbox 360 Kinect 获取的 3D 数据集上进行了评估，并取得了优异的结果。

![image-20230804104719190](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230804104719190.png)

- ​															图 1.我们的人体检测方法概述。

我们的算法仅利用深度信息。它还可以与传统的基于梯度的方法相结合，以提供更快、更准确的检测。该检测算法还可以作为使用深度信息进行姿态估计、跟踪或活动识别研究的第一步。我们的论文的结构安排如下。第 2 节概述了我们的方法。第 3 节描述了 2D 倒角距离匹配。第 4 节描述了 3D 模型拟合。第 5 节详细介绍了提取人物的整体轮廓。第 6 节给出了我们对跟踪的初步研究。第 7 节讨论实验结果。第 8 节总结了本文并给出了未来研究的可能方向。

## 2.方法概述

本节概述了我们方法的主要步骤，如图1所示。第 3 节到第 6 节介绍了实现细节。给定输入深度数组，我们首先减少噪声并平滑数组以供后续处理。我们使用两阶段头部检测过程来定位人员。我们首先探索嵌入在深度数组中的边界信息，以定位可能指示人的外观的候选区域。这里使用的算法是2D倒角距离匹配。它扫描整个图像并给出可能包含人物的区域。我们使用 3D 头部模型检查每个区域，该模型利用阵列的相关深度信息进行验证。我们从深度数组中提取头部的参数，并使用该参数构建 3D 头部模型。然后我们将 3D 模型与所有检测到的区域进行匹配以做出最终估计。我们还开发了一种区域生长算法来找到人的整个身体并提取他/她的整个身体轮廓。为了准确地做到这一点，我们提出了一种分割方法来分割身体和与之相连的物体。外，我们还展示了一些使用我们的检测结果进行跟踪的初步研究。

## 3、二维倒角距离匹配 

### 3.1．预处理 

为了准备用于处理的数据，需要进行一些基本的预处理。在 Kinect 拍摄的深度图像中，传感器无法测量深度的所有点在输出数组中都偏移为 0。我们将其视为一种噪音。为了避免它的干扰，我们要恢复它的真实深度值。我们假设空间是连续的，并且缺失点更有可能与其邻居具有相似的深度值。有了这个假设，我们将所有 0 像素视为空缺，需要进行填充。我们使用最近邻插值算法来填充这些像素，并得到一个在所有像素中都具有有意义值的深度数组。然后我们在深度数组上使用带有 4×4 窗口的中值滤波器来使数据平滑。

###  3.2. 2D chamfer distance matching [2D 倒角距离匹配]

 该方法的第一阶段是使用嵌入在深度数组中的边缘信息来定位可能指示人的外观的可能区域。这是一种粗略的扫描方法，因为我们需要得到一个假阴性率尽可能低的粗略检测结果，但可能会有相对较高的假阳性率提供给下一阶段。我们在此阶段使用2D倒角距离匹配来快速处理。此外，倒角距离匹配是一种良好的二维形状匹配算法，具有缩放不变性，并且它利用深度数组中的边缘信息，即场景中所有对象的边界。我们使用 Canny 边缘检测器来查找深度数组中的所有边缘。为了减少计算量并减少周围不规则物体的干扰，我们消除了所有尺寸小于某个阈值的边缘。 （这里，边缘的大小由其包含的像素决定。）倒角距离匹配的结果如图2所示。

![图 2](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230804105644916.png)

​				**图 2. 2D 倒角匹配的中间结果。 (a)显示降噪后的深度阵列。 (b) 给出了使用 Canny 边缘检测器计算的二值边缘图像，然后消除小边缘。 (c) 显示了从边缘图像 (b) 生成的距离图。将二进制头部模板 (d) 与 (c) 进行匹配，得到头部检测结果 (e)。黄点表示检测到的位置。**

我们使用图 2（d）所示的二进制头部模板，并将该模板与生成的边缘图像进行匹配。为了提高效率，在匹配过程之前计算距离变换。这会产生边缘图像的距离图**[距离图是指在图像处理中，计算每个像素到最近目标或边缘的距离，并将距离值表示为图像的像素强度。这样可以得到一个灰度图像，其中像素的值表示该像素到最近目标或边缘的距离。计算距离图的常用方法之一是距离变换算法，它可以根据像素之间的距离计算出每个像素的距离值。OpenCV库中提供了`cv2.distanceTransform`函数用于执行距离变换。]**，其中像素包含到边缘图像中最近的数据像素的距离。匹配包括将模板平移并定位在距离图的各个位置；匹配度量由位于变换模板的数据像素下方的距离图像的像素值确定。这些值越低，该位置的图像和模板之间的匹配越好。如果距离值低于某个阈值，则认为在该位置检测到目标物体，这意味着在此发现了头状物体。我们在这里使用短语“类似头部的物体”，因为我们检测到的物体可能不是真正的头部，因为我们在这里使用了很高的阈值来保证非常低的漏报率。

这个物体是否真的是一个头，我们将在下一阶段决定。通常情况下，场景中的人都可能出现在任意深度，这意味着头部大小会根据深度而变化。为了使算法具有缩放不变性，我们生成一个图像金字塔，原始图像位于底部；对每个图像进行子采样以生成更高级别的下一个图像。我们这里使用的子采样率为3/4，金字塔的层数取决于场景。如果场景包含更大的深度范围，则需要更多的级别。该模板能够在所有姿势和视图中找到人的头部。如果人处于水平位置或倒置，通过旋转模板并运行相同的检测过程可以轻松解决。该阶段所有步骤的结果如图2所示。

## 4、3D模型拟合

现在我们将检查 2D 倒角匹配算法检测到的所有区域。

### 4.1.计算头部参数

为了生成适合深度数组的 3D 模型，我们必须知道检测区域中出现的头部的真实参数。为了估计这一点，我们进行了实验并得到了头部深度和高度的回归结果，如图3所示。

![Fig. Regression result of head height and depth.](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230804111310761.png)

**图3 头部高度和深度的回归结果。**

我们得到的三次方程是：
$$
-y=p_1 \cdot x^3+p_2 \cdot x^2+p_3 \cdot x+p_4
\tag{1}
\\\mathbf{Here},\\
 \begin{cases}
p_1=-1.3835 \times 10^{-9}\\
p_2=1.8435 \times 10^{-5}\\
p_3=-0.091403\\
p_4=189.38\\
\end{cases}
$$
根据2D倒角匹配的检测结果，我们可以从原始深度数组中得到头部类物体的深度。通过式（1），我们计算出该深度下头部的标准高度。然后我们在头部定义的一定范围内搜索头部：
$$
R=1.33\frac{h}{2}
\tag{2}
$$
这里，**h**是头部计算公式(1)的高度，R是搜索半径。接下来，我们在边缘图像中搜索由半径 **R** 定义的头部。

接下来，我们在边缘图像中由半径 R 定义的圆形区域内搜索头部。如果该区域中存在满足所有约束的圆形边，例如size通过一定的阈值，就判定检测到了头部。接下来要做的就是找到头部的真实半径。恰好我们在2D倒角匹配阶段计算的距离图可以用来估计头部的半径。回想一下，距离图中的像素包含从该像素到边缘图像中最近的数据像素的距离，考虑到头部是圆形的，距离图中头部中心的值只是头部半径的近似值。所以我们可以直接以此作为我们对头部真实半径$R_t$的估计。

### 4.2.生成3D模型

考虑到3D模型拟合的计算复杂度较高，我们希望模型是视图不变的，这样我们就不必使用多个不同的模型或旋转模型并运行多次。该模型应从所有视图概括头部的特征：前视图、后视图、侧视图以及当传感器放置得较高或较低或人处于较高或较低位置时的较高和较低视图。为了满足这些限制并使其最简单，我们使用半球作为 3D 头部模型。

### 4.3. Fitting

接下来，我们将模型拟合到之前步骤中检测到的区域。我们提取一个围绕检测中心半径为 $R_t$的圆形区域 **CR** 并对其深度进行归一化：
$$
depth_n(i,j)=depth(i,j)-min_{i,j}(depth(i,j))\\
i,j\in CR 
\tag{3}
$$
这里，深度$(i,j)$是深度数组中像素$（i，j）$的深度值。深度$depth_n(i,j)$是像素$（i，j）$的归一化深度值。然后我们计算圆形区域与3D之间的平方误差模型：
$$
Er=\sum_{i,j \in CR}|depth_n(i,j)-template(i,j)|^2
\tag{4}
$$
![image-20230804113412709](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230804113412709.png)

**图4. 3D头部模型。 (a) 说明了头部模型的要求：模型应该对不同视图保持不变。(b) 显示我们用作 3D 头部模型的半球模型**

我们使用阈值来确定该区域是否实际上是头部。图5说明了该阶段的一些步骤，并显示了3D匹配的结果。

## 5.提取轮廓

我们提取人的整体轮廓，以便我们可以跟踪他/她的手和脚并识别活动。在 RGB 图像中，尽管人站在地面上，但检测脚和地面之间的边界不成问题然而，在深度数组中，人脚处的值和局部地平面的值是相同的。因此，使用常规边缘检测器从深度阵列计算人体的全身轮廓是不可行的。当人触摸与人部分处于相同深度的任何其他物体时，这同样适用。为了解决这个问题，我们利用了这样一个事实：无论姿势如何，人的脚通常在深度阵列中呈现直立状态，我们使用下面的滤波器响应去提取人体与地面的边界。
$$
F=[1,1,1,-1,-1,-1]^T
\tag{5}
$$
![image-20230804113759275](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230804113759275.png)

**图5（a）说明了从距离图估计头部真实参数的过程。 3D模型拟合的输入是图2（e）中2D倒角匹配的输出。 3D 模型拟合的输出如 (b) 所示。黄点表示检测到的头部中心。**

阈值滤波器响应描绘了与地板平行的平面区域。 F 滤波器响应提取的边缘与原始深度数组一起添加在一起，作为区域生长算法的输入。图 6 显示了滤波器响应的示例。 （两幅图像中的颜色分布略有不同，因为我们缩放了显示数组，相应的值是相同的。）

![image-20230804113844151](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230804113844151.png)

图 6.(a) 原始深度数组。身体的某些部分与地平面和墙壁合并。 (b) 区域生长算法的输入深度数组。接地层由阈值 F 滤波器响应描绘。脚部的边缘将人与地板很好地分开

我们开发了一种区域生长算法来从处理后的深度数组中提取全身轮廓。假设人体表面的深度值是连续的并且仅在特定范围内变化。该算法从种子位置开始，它是 3D 模型拟合检测到的区域的质心。区域生长的规则基于该区域与其相邻像素之间的相似性。深度数组中两个像素 x 和 y 之间的相似度定义为：
$$
S(x,y)=|depth(x)-depth(y)|
\tag{6}
$$

| 开始区域生长，直到区域与相邻像素之间的相似度高于阈值         |
| :----------------------------------------------------------- |
| 1. 初始化：区域=种子<br />2.<br />     (1)查找该区域的所有相邻像素<br />     (2)测量像素与区域（Eq.7）s1，s2，…的相似度，并根据相似度对像素进行排序。<br/>     (3)如果 smin < 阈值<br/>          (3.1)添加与该区域相似度最高的像素。<br />          (3.2)计算该区域的新平均深度。<br />          (3.3)Repeat (1)-(3)<br />         else:<br />                算法终止<br />3.返回区域 |

**表 1. 区域生长算法**

这里，$S$ 是相似度，$depth()$返回像素的深度值。区域的深度由该区域中所有像素的平均深度定义：
$$
depth(R)=\frac{1}{N}\sum_{i \in R}(depth(i))
$$
我们的算法的伪代码总结在表1中。我们的区域生长算法的结果如图7所示。

![image-20230804120009950](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230804120009950.png)

**图 7.(a) 我们的区域增长算法的结果。 (b) 将提取的全身轮廓叠加在深度图上**

# 结合深度信息的图像分割算法研究

# 深度图像边缘检测的简单实现

深度图像边缘检测的简单实现

### 1.深度差值检测

相邻像素点之间的深度测量值存在较大差值，这时认为这两点在三维空间是不连续的。
我们使用如下方法检测：
$$
p_1=\sum_{l=-1,k=-1}^{l=1,k=1}\mathbf{abs[depth(i,j)-depth(i+l,j+k)]}
$$
如果$p_1 > \mathbf{threshold_p*depth(i,j)}，$则该点是断崖点。

### 2.梯度差值检测

相邻像素点之间的深度梯度测量值发生较大变化，这时认为这两点可能位于深度图像的轮廓线上(即不在平面上)。
例如：两面墙相交的轮廓线。线上点a在线两侧相邻的点b、c的梯度分解存在方向相反的向量，利用这个特性来检测。

#### 方法1： - > 阈值

$$
p_2=\sum_{l=-1,k=-1}^{l=1,k=1}\mathbf{abs}[depth_x(i-l,j-k)-depth_x(i+l,j+k)]\\ + \sum_{l=-1,k=-1}^{l=1,k=1}\mathbf{abs}[depth_y(i-l,j-k)-depth_y(i+l,k+l)]
$$

如果$p_2 > threshold_p*depth(i,j)$，则该点不在平面上。
方法2： $\bar{ab}\cdot \bar{bc}<0(夹角>90^。)$
$$
p_2=depth_x(i-l,j-k)*depth_x(i+l,j+k)+depth_y(i-l,j-k)*depth_y(i+l,k+l)]
$$
如果p2 < 0，则该点不在平面上。

