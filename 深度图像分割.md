#  [Segmenting Unknown 3D Objects from Real Depth Images using Mask R-CNN Trained on Synthetic Data](https://arxiv.org/pdf/1809.05825.pdf)

code:https://github.com/BerkeleyAutomation/sd-maskrcnn

总结
感觉这篇文章就是把maskrcnn用在了depth上，大数据的训练实现了class agnostic。本文的一个亮点是提供了一个数据集，该数据集既有合成的，又有对应真实数据集。
既然简单地分成了前景和背景，那么其他的检测？分割的方法也可以这么做。
而且只是要简单地分割成前景背景，太多的rgb信息反而容易形成混淆。深度图训练效果会比较好。

3.2.1 WISDOM-Sim和WISDOM-Real
3.2.2 Synthetic Depth MASK R-CNN(SD Mask-rcnn)
1 将深度图看成灰度图，改成三通道。
2 将类分成两类，前景和背景
3 depth输入改成512*512
4 网络主框架从ResNet 101改成ResNet 35
5 设置mean pixel value

![在这里插入图片描述](https://img-blog.csdnimg.cn/20200418114548561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2VpZ2h0X0plc3Nlbg==,size_16,color_FFFFFF,t_70)

从这张图看，但训练的图片越多，还有类别越多，效果会越好。




# Deep Learning for RGB-D Image Segmentation

![image-20230710231950617](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230710231950617.png)

## 从 RGB-D 图像中学习丰富的特征以进行对象检测和分割，ECCV 2014

概述：我们的系统从 RGB 和深度图像对中检测轮廓，生成 2.5D 区域建议，将它们分类为对象类别，然后推断“事物”类对象实例的分割掩模，以及属于以下对象的像素的标签：类似“东西”的类别。

我们建议在每个像素处使用三个通道对深度图像进行编码：水平视差**(horizontal disparity)**、地面高度**(height above ground)**以及像素局部表面法线与推断重力方向形成的角度。我们将此编码称为 **HHA**。所有三个通道均进行线性缩放，以将训练数据集中的观测值映射到 0 到 255 的范围。

HHA 表示对地心姿态的属性进行编码，强调图像中的互补不连续性（深度、表面法线和高度）。此外，CNN 不太可能自动学习直接从深度图像计算这些属性，特别是当可用的训练数据非常有限时。

我们的假设是，为 RGB 图像设计的网络也可以学习 HHA 图像的合适表示。

## FuseNet：通过基于融合的 CNN 架构将深度融入语义分割

随着 RGB-D 相机的出现，预计额外的深度测量将提高精度。在这里，我们研究了如何利用卷积神经网络（CNN）将互补深度信息合并到语义分割框架中的解决方案。最近，编码器-解码器类型的全卷积 CNN 架构在语义分割领域取得了巨大成功。受此观察的启发，我们提出了一种编码器-解码器类型的网络，其中编码器部分由网络的两个分支组成，这两个分支同时从 RGB 和深度图像中提取特征，并随着网络的深入将深度特征融合到 RGB 特征图中。

![image-20230711172128459](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711172128459.png)

FuseNet 的示例输出。从左到右：输入RGB和深度图像，预测的语义标签和对应标签的概率，其中白色和蓝色分别表示高概率和低概率![image-20230711172516812](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711172516812.png)

所提出的 FuseNet 的架构。颜色表示图层类型。该网络包含两个分支，用于从 RGB 和深度图像中提取特征，并且深度的特征图不断融合到 RGB 分支中，用红色箭头表示。在我们的架构中，融合层被实现为逐元素求和，如虚线框中所示。

FuseNet 由编码器-解码器类型的网络架构组成。该网络有两个主要部分：

1. 编码器部分提取特征；
2.  解码器部分将特征映射上采样回原始输入分辨率。

这种编码器-解码器风格已经在 DeconvNet 和 SegNet 等之前的一些工作中引入，并取得了良好的分割性能。此外，两个分支从 RGB 和深度图像中提取特征。我们注意到深度图像被归一化为与彩色图像具有相同的值范围，即进入 [0,255] 区间。为了结合来自两个输入模块的信息，我们将深度分支的特征图融合到 RGB 分支的特征图中。

FuseNet 的编码器部分类似于 16 层 VGG 网络，除了全连接层 fc6、fc7 和 fc8 之外，因为全连接层将分辨率降低了 49 倍，这增加了上采样部分的难度。在我们的网络中，我们总是在卷积（Conv）之后和修正线性单元1（ReLU）之前使用批量归一化（BN）来减少内部协变量偏移。我们将卷积、批量归一化和 ReLU 的组合分别称为 CBR 块。 BN 层首先将特征图归一化为零均值和单位方差，然后对其进行缩放和移位。特别是，尺度和平移参数是在训练期间学习的。因此，颜色特征不会被深度特征覆盖，但网络会学习如何以最佳方式组合它们。

解码器部分是编码器部分的对应部分，其中应用记忆的反池化来对特征图进行上采样。在解码器部分，我们再次使用 CBR 块。我们还用反卷积代替卷积进行了实验，并观察到非常相似的性能。

![image-20230711172713300](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711172713300.png)

VGG 16 层网络的第二个 (CBR2) 和第三个 (CBR3) 卷积块的不同融合策略的图示。 (a) 融合层仅插入在每个池化层之前。 (b) 在每个 CBR 块之后插入融合层。![image-20230711172739288](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711172739288.png)

Huang 等人提出的基于 3DCNN 的点云语义标记系统。云经历密集体素化过程，CNN 生成每体素标签，然后将其映射回点云。

## Deep Projective 3D Semantic Segmentation深度投影 3D 语义分割，2017

3D 点云的语义分割对于许多实际应用来说是一个具有挑战性的问题。虽然深度学习彻底改变了图像语义分割领域，但迄今为止其对点云数据的影响有限。最近基于 3D 深度学习方法 (3DCNN) 的尝试取得了低于预期的结果。此类方法需要对底层点云数据进行体素化，从而导致空间分辨率降低和内存消耗增加。此外，3D-CNN 还受到注释数据集可用性有限的影响。

在本文中，我们提出了一种替代框架，可以避免 3D-CNN 的局限性。我们没有直接解决 3D 问题，而是首先将点云投影到一组合成 2D 图像上。然后将这些图像用作 2D-CNN 的输入，该 2D-CNN 专为语义分割而设计。最后将得到的预测分数重新投影到点云上，得到分割结果。我们进一步研究了多流网络架构中多种模式的影响，例如颜色、深度和表面法线。![image-20230711172953685](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711172953685.png)

所提出方法的概述。输入点云被投影到多个虚拟摄像机视图中，生成 2D 颜色、深度和表面法线图像。每个视图的图像由多流 CNN 处理以进行语义分割。所有视图的输出预测分数被融合为每个点的单个预测，从而产生点云的 3D 语义分割。

![image-20230711173048393](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711173048393.png)

所提出的用于 2D 语义分割的多流架构的图示。每个输入流都由全卷积网络处理。将每个流的预测分数相加即可得到最终预测。

## Depth-aware CNN for RGB-D Segmentation, 2018

由于固定的网格内核结构，卷积神经网络（CNN）由于缺乏处理几何信息的能力而受到限制。深度数据的可用性使得使用 CNN 的 RGB-D 语义分割取得了进展。最先进的方法要么使用深度作为附加图像，要么处理 3D 体积或点云中的空间信息。这些方法的计算和内存成本很高。

为了解决这些问题，我们通过引入两种直观、灵活和有效的操作来提出深度感知 CNN：深度感知卷积和深度感知平均池化。通过利用信息传播过程中像素之间的深度相似性，几何形状可以无缝地融入到 CNN 中。在不引入任何额外参数的情况下，这两个算子都可以轻松集成到现有的 CNN 中。

![image-20230711173224678](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711173224678.png)

深度感知 CNN 的插图。 A 和 C 被标记为桌子，B 被标记为椅子。它们在 RGB 图像中都具有相似的视觉特征，但在深度上是可分离的。深度感知 CNN 在卷积和池化中结合了像素的几何关系。当A是感受野的中心时，C对输出单元的贡献比B更大。最右一列的图显示了深度感知CNN的RGBD语义分割结果。

引入了两个新运算符：（a）深度感知卷积和（b）深度感知平均池化。

深度感知卷积通过深度相似性项增强了标准卷积。我们强制与内核中心深度相似的像素比其他像素对输出有更多贡献。这个简单的深度相似性项有效地将几何结构合并到卷积核中，并有助于构建深度感知的感受野，其中卷积不受固定网格几何结构的限制。

第二个引入的运算符是深度软件平均池化。类似地，当对特征图的局部区域应用滤波器时，在计算局部区域的平均值时考虑相邻像素之间的深度成对关系。视觉特征能够沿着深度图像中给出的几何结构传播。这种几何感知操作使得能够利用深度图像来定位对象边界。

![image-20230711173302403](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711173302403.png)

深度感知 CNN 中信息传播的图示。不失一般性，我们只显示一个核大小为 3 × 3 的滤波器窗口。在图中所示的深度相似度中，较深的颜色表示相似度较高，而较浅的颜色表示两个像素在深度上相似度较低。在（a）中，深度感知卷积的输出激活是深度相似度窗口与输入特征图上的卷积窗口的乘积。与(b)中类似，深度感知平均池化的输出是根据深度相似度加权的输入窗口的平均值。

## Cascaded Feature Network for Semantic Segmentation of RGB-D Images, ICCV 

## 用于 RGB-D 图像语义分割的级联特征网络，ICCV 2017![image-20230711173349462](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711173349462.png)

深度和场景分辨率之间存在相关性：近场（以蓝色矩形突出显示）包含高场景分辨率，而远场（以红色矩形突出显示）具有低场景分辨率。这项工作的关键思想是使用深度将图像分割成代表相似视觉特征的层，或“场景分辨率”，例如一般物体和场景的分辨率。

深度和场景分辨率之间存在相关性：较低的场景分辨率出现在具有较高深度的区域中，而较高的场景分辨率出现在近场中。在较低场景分辨率区域中，对象和场景密集共存，相对于较高场景分辨率区域，对象/场景之间形成更复杂的相关性。为了更好地表示和学习不同的对象/场景关系，应该为不同的场景分辨率构建适当的特征。

首先，为了使特征更加关注观察场景的共同视觉特征，引入了上下文感知感受野（CaRF）。 CaRF 可以更好地控制所学习特征的相关上下文信息。CaRF 是基于超像素计算的，超像素由底层场景结构定义。因此，CaRF提供的上下文信息可以减轻混合过小或过大区域特征的负面影响。

其次，开发了具有并行分支的级联特征网络（CFN），每个分支都专注于特定场景分辨率区域的语义分割。每个分支都配备了 CaRF。 CaRF和级联网络的结合，使得不同场景分辨率的区域能够相互通信，从而明智地更新共享的卷积特征。![image-20230711173657074](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711173657074.png)

我们的级联特征网络（CFN）的概述。给定彩色图像，我们使用 CNN 计算卷积特征图。离散深度图像是分层的，其中每一层代表场景分辨率，并用于将图像区域与共享相同卷积特征图的相应网络分支进行匹配。每个分支都有上下文感知感受野（CaRF），它产生上下文表示以与相邻分支的特征相结合。所有分支的预测被结合起来以获得最终的分割结果。

![image-20230711173726757](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711173726757.png)

两级上下文感知感受野（CaRF）：（a）将图像划分为不同大小的超像素； (b) 在粗网格的每个节点，我们聚合驻留在同一超像素中的特征； (c)聚合相邻超像素的内容； (d) 特征图中的聚合内容表示 CaRF。两级 CaRF 被重复应用于由不同尺寸的超像素划分的图像。请注意，由于网络的下采样，特征图的分辨率小于图像。

![image-20230711173752578](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711173752578.png)

该网络可以具有单独的分支（a）、组合分支（b）或级联分支（c）。为了清楚起见，我们仅用两个分支进行说明。每个网络都可以扩展以拥有更多分支。

上下文感知感受野（CaRF）的思想是将局部上下文的卷积特征聚合成更丰富的特征，以更好地学习相关内容，其中感受野是空间变化的，并根据局部上下文定义其范围。![image-20230711173827462](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711173827462.png)

## Multi-View Deep Learning for Consistent Semantic Mapping with RGB-D Cameras, 使用 RGB-D 相机进行一致语义映射的多视图深度学习,2017

开发了一种新颖的深度学习方法，用于对具有多视图上下文的 RGB-D 图像进行语义分割，其中 RGB 和深度融合通过多尺度深度监督和多视图一致性约束无缝融合。一个共同的原则是使用 SLAM 轨迹估计将多个帧的网络输出扭曲为带有地面实况注释的参考视图。通过这种方式，网络可以学习在视点变化下不变的特征。![image-20230711173919926](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711173919926.png)

关键的创新是通过使用 SLAM 轨迹将 CNN 特征图从多个视图变形为公共参考视图来增强一致性，并监督多个尺度的训练。我们的方法提高了单视图分割的性能，并且特别有利于多视图融合分割。

![image-20230711174046296](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711174046296.png)

我们的方法中使用的 CNN 编码器-解码器架构。网络的输入是 RGB-D 序列以及来自 SLAM 轨迹的相应姿势。受 FuseNet 启发，编码器包含两个分支来从 RGB-D 数据中学习特征。

获得的低分辨率高维特征图通过解码器中的反卷积连续细化。我们将特征映射转换为公共参考视图，并通过各种约束强制执行多视图一致性。网络以深度监督的方式进行训练，其中在解码器的所有尺度上计算损失

![image-20230711174111290](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711174111290.png)

![image-20230711174147594](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711174147594.png)

具有来自 RGB 和下采样 HHA 编码深度的 CNN 特征的检测管道。 C 表示最后一个卷积层之后的 CNN 特征图数量（VGG-16 为 512）。内部提案生成器生成 B 提案（1000）。

![image-20230711174230251](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711174230251.png)

具有来自 RGB 和 HHA 编码深度的级联 CNN 特征的检测管道。 C 表示最后一个卷积层之后的 CNN 特征图数量（VGG16 为 512）。内部提案生成器生成 B 提案（1000）。对于跨模态蒸馏方法，训练 CNN ψ 来模仿预训练的 CNN φ。![image-20230711174259627](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711174259627.png)

## RGB-D Face Recognition via Deep Complementary and Common Feature Learning, 通过深度互补和共同特征学习进行 RGB-D 人脸识别，2018

![image-20230711174332165](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711174332165.png)

RGB-D人脸识别（FR）由两种典型的场景组成：（1）多模态匹配，例如RGB-D探针与RGB-D图库，其中注册图库和探针图像都是使用RGBD传感器捕获的， (2) 跨模态匹配，例如 RGB 探针与 RGB-D 图库，其中图库图像保持 RGB-D，但探针图像由 RGB 传感器捕获。所提出的方法通过学习互补和共同特征来解决这两个问题。

![image-20230711174402554](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711174402554.png)

概述了所提出的 RGB 和深度模态的互补特征学习方法，该方法处理多模态 FR 场景，例如 RGB-D 探针与 RGB-D 图库。

![image-20230711174518016](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711174518016.png)

我们的互补特征学习的网络架构

![image-20230711174540174](C:\Users\asus\AppData\Roaming\Typora\typora-user-images\image-20230711174540174.png)

引入loss4单元的细节是为了加强互补特征学习。加号表示逐元素平均值。